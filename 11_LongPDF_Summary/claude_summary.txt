TinyLlama is an open-source 1.1B-parameter language model pretrained on about 1 trillion tokens for 3 epochs. It adopts the Llama 2 architecture and tokenizer while leveraging optimizations like FlashAttention to improve efficiency. Despite its compact size, TinyLlama demonstrates competitive performance on commonsense reasoning and problem-solving benchmarks, often surpassing similarly-sized models like OPT-1.3B and Pythia-1.4B. The model checkpoints and code are publicly available, aiming to facilitate language model research with its strong results and lightweight footprint.